<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pagina web DEGA</title>
    <link rel="shortcut icon" href="img/Icono_arriba.png" type="image/x-icon">
    <link rel="stylesheet" href="css/estilos.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Scheherazade+New&display=swap" rel="stylesheet">
</head>

<body>
    <header>
        <section class="nav">
            <li>  <a href="index.html">Inicio</a></li>
           <li> <a href="">Portafolio</a>
           <ul>
            <li><a href="unidad1.html">Unidad 1</a></li>
            <li><a href="unidad2.html">Unidad 2</a></li>
            <li><a href="unidad3.html">Unidad 3</a></li>
            <li><a href="unidad4.html">Unidad 4</a></li>
        </ul></li>
        <li>  <a href="presentaciones.html">Presentaciones</a></li>
            <li>  <a href="trabajos.html">Trabajos</a></li>
            
        </section>
        <section class="textos-header">
            <h1>
                Unidad 4
            </h1>
            <h2>
                Procesamiento paralelo
            </h2>
        </section>
        <div class="wave" style="height: 150px; overflow: hidden;">
            <svg viewBox="0 0 500 150" preserveAspectRatio="none" style="height: 100%; width: 100%;">
                <path d="M0.00,49.98 C149.83,123.84 343.95,-34.03 500.00,49.98 L500.00,150.00 L-24.54,177.13 Z"
                    style="stroke: none; fill: rgb(255, 255, 255);">

                </path>
            </svg>
        </div>
    </header>

    <main>
        <section class="indice" >
            <div class="ind-box">
                <p class="ind-titulo" aling="center">Contenido</p>
                <ul class="Ind-lista">
                    <li><a href="unidad4html"> 1 </a> </li>
                    <ul>
                        <li><a href="#aspectos basicos de la computacion paralela">Aspectos basicos de la computacion paralela</a></li>
                        <li><a href="#tipos de computacion paralela">Tipos de computacion paralela</a></li>
                        <li><a href="#clasificacion">Clasificacion</a></li>
                        <li><a href="#arquitectura de computadores secuenciales">Arquitectura de computadores secuenciales</a></li>
                        <li><a href="#organizacion de direcciones de memoria">Organizacion de direcciones de memoria</a></li>
                        
                    </ul>
                    <li><a href="unidad4.html"> 2</a> </li>
                    <ul>
                        <li><a href="#sistemas de memoria (compartida) multiprocesadores">Sistemas de memoria (compartida) multiprocesadore</a></li>
                        <li><a href="#redes de interconexion dinamica (indirecta) medio compatido conmutadas">Redes de interconexion dinamica (indirecta) medio compatido conmutadas</a></li>
                        
                       
                      
                        <li><a href="unidad4.html"> 3 </a> </li>
                        <ul>
                            <li><a href="#sistemas de memoria distribuida multicomputadoreso">Sistemas de memoria distribuida multicomputadoreso</a></li>
                            <li><a href="#redes de interconexion estaticas">Redes de interconexion estaticas</a></li>
                            
                           
                        </ul>

                        <li><a href="unidad4.html"> 4</a> </li>
                        <ul>
                            <li><a href="casos para estudio">Casos para estudio</a></li>
                            
                            
                           
                        </ul>
                    
                    
                </ul>
            </div>
        </section>

       
        <section class="contenedor sobre-el-curso">
            
            <div class="contenedor-sobre-el-curso">
             
                <div class="contenido-textos">
                 
                    <h3 id="aspectos basicos de la computacion paralela"><span>4.1</span>Aspectos basicos de la computacion paralela</h3>
                    <p>
                        La computación paralela es una forma de cómputo en la que muchas instrucciones se ejecutan
                simultáneamente,
                operando sobre el principio de que problemas grandes, a menudo se pueden
                    dividir en unos más pequeños, que luego son resueltos simultáneamente (en paralelo). Hay varias
                    formas diferentes de computación paralela: paralelismo a nivel de bit, paralelismo a nivel de
                    instrucción, paralelismo de datos y paralelismo de tareas. El paralelismo se ha empleado durante
                muchos años, sobre todo en la computación de altas prestaciones, pero el interés en ella ha
                    crecido últimamente debido a las limitaciones físicas que impiden el aumento de la frecuencia.n.
                 Como el consumo de energía —y por consiguiente la generación de calor— de las
                    computadoras constituye una preocupación en los últimos años,n. 
                la computación en paralelo se
                    ha convertido en el paradigma dominante en la arquitectura de computadores, principalmente en
                    forma de procesadoresmultinúcleo
                    </p>
                    <h3 id="tipos de computacion paralela"><span>4.2</span>Tipos de computacion paralela</h3>
                    <p> 
                        Paralelismo a nivel de bit:<br>
                        Desde el advenimiento de la integración a gran escala (VLSI) como tecnología de fabricación de chips de computadora en la década de 1970 hasta alrededor de 1986, la aceleración en la arquitectura de computadores se lograba en gran medida duplicando el tamaño de la palabra en la computadora, la cantidad de información que el procesador puede manejar por ciclo. 
                        <br>
                        El aumento del tamaño de la palabra reduce el número de instrucciones que el procesador debe ejecutar para realizar una operación en variables cuyos tamaños son mayores que la longitud de la palabra. Por ejemplo, cuando un procesador de 8 bits debe sumar dos enteros de 16 bits, el procesador primero debe adicionar los 8 bits de orden inferior de cada número entero con la instrucción de adición, a continuación, añadir los 8 bits de orden superior utilizando la instrucción de adición con acarreo que tiene en cuenta el bit de acarreo de la adición de orden inferior, en este caso un procesador de 8 bits requiere dos instrucciones para completar una sola operación, en donde un procesador de 16 bits necesita una sola instrucción para poder completarla.
                        <br>
                        Históricamente, los microprocesadores de 4 bits fueron sustituidos por unos de 8 bits, luego de 16 bits y 32 bits, esta tendencia general llegó a su fin con la introducción de procesadores de 64 bits, lo que ha sido un estándar en la computación de propósito general durante la última década.
                        <br><br>
                        Paralelismo a nivel de instrucción:<br>
                        Un programa de ordenador es, en esencia, una secuencia de instrucciones ejecutadas por un procesador. Estas instrucciones pueden reordenarse y combinarse en grupos que luego son ejecutadas en paralelo sin cambiar el resultado del programa. Esto se conoce como paralelismo a nivel de instrucción. Los avances en el paralelismo a nivel de instrucción dominaron la arquitectura de computadores desde mediados de 1980 hasta mediados de la década de 1990.
                        <br>
                        Los procesadores modernos tienen ''pipeline'' de instrucciones de varias etapas. Cada etapa en el pipeline corresponde a una acción diferente que el procesador realiza en la instrucción correspondiente a la etapa; un procesador con un pipelinede N etapas puede tener hasta n instrucciones diferentes en diferentes etapas de finalización. El ejemplo canónico de un procesador segmentado es un procesador RISC, con cinco etapas: pedir instrucción, decodificar, ejecutar, acceso a la memoria y escritura. El procesador Pentium 4 tenía un pipeline de 35 etapas.
                        <br>
                        Además del paralelismo a nivel de instrucción del pipelining, algunos procesadores pueden ejecutar más de una instrucción a la vez. Estos son conocidos como procesadores superescalares. Las instrucciones pueden agruparse juntas sólo si no hay dependencia de datos entre ellas. El scoreboarding y el algoritmo de Tomasulo —que es similar a scoreboarding pero hace uso del renombre de registros— son dos de las técnicas más comunes para implementar la ejecución fuera de orden y la paralelización a nivel de instrucción.
                        <br>
                        Un pipeline canónico de cinco etapas en una máquina RISC (IF = Pedido de Instrucción, ID = Decodificación de instrucción, EX = Ejecutar, MEM = Acceso a la memoria, WB = Escritura).
                        <br><br>
                        Paralelismo de datos:<br>
                        El paralelismo de datos es el paralelismo inherente en programas con ciclos, que se centra en la distribución de los datos entre los diferentes nodos computacionales que deben tratarse en paralelo. La paralelización de ciclos conduce a menudo a secuencias similares de operaciones —no necesariamente idénticas— o funciones que se realizan en los elementos de una gran estructura de datos. Muchas de las aplicaciones científicas y de ingeniería muestran paralelismo de datos.
                        <br>
                        Una dependencia de terminación de ciclo es la dependencia de una iteración de un ciclo en la salida de una o más iteraciones anteriores. Las dependencias de terminación de ciclo evitan la paralelización de ciclos.
                        <br>
                        Un procesador superescalar con pipeline de cinco etapas, capaz de ejecutar dos instrucciones por ciclo. Puede tener dos instrucciones en cada etapa del pipeline, para un total de hasta 10 instrucciones (se muestra en verde) ejecutadas simultáneamente.
                        <br><br>
                        Paralelismo de tareas:<br>
                        El paralelismo de tareas es la característica de un programa paralelo en la que cálculos completamente diferentes se pueden realizar en cualquier conjunto igual o diferente de datos. Esto contrasta con el paralelismo de datos, donde se realiza el mismo cálculo en distintos o mismos grupos de datos. El paralelismo de tareas por lo general no escala con el tamaño de un problema.
                    </p>
                    <h3 id="clasificacion"><span>4.2.1</span>Clasificacion</h3>
                    <p>HQOLAA</p>
                    <h3 id="arquitectura de computadores secuenciales"><span>4.2.2</span>Arquitectura de computadores secuenciales</h3>
                    <p>
                        Una celda de memoria es un circuito electrónico que recuerda un valor de entrada después que dicho
                         valor ha desaparecido. La unidad de memoria más básica es el flip-flop Set/Reset. Aunque recordar un bit sencillo 
                         es importante, la mayoría de los sistemas de cómputo requieren recordar un grupo de bits, esto se logra combinando 
                         varios flip-flop en paralelo, una conexión de este tipo recibe el nombre de registro. A partir de aquí es posible
                          implementar diferentes circuitos como registros de corrimiento y 
                        contadores, éstos últimos también se conocen como circuitos de reloj. Con los elementos mencionados es posible 
                        construir un microprocesador completo.
                    </p>
                    <h3 id="organizacion de direcciones de memoria"><span>4.2.3</span>Organizacion de direcciones de memoria</h3>
                    <p>
                        Es un formato de localización de bytes de memoria con la cual un programa 
                        informático o un dispositivo de hardware accede o almacena datos para su posterior utilización.

                        Una forma común de describir la memoria principal de un ordenador es 
                        como una colección de celdas que almacenan datos e instrucciones. Cada celda está identificada unívocamente por un número o dirección de memoria.
                    </p>
           
                </div>
            </div>
        </section>

        <section class="contenedor sobre-el-curso">
            
            <div class="contenedor-sobre-el-curso">
             
                <div class="contenido-textos">
                 
                    <h3 id="sistemas de memoria (compartida) multiprocesadores"><span>4.3</span>Sistemas de memoria (compartida) multiprocesadores</h3>
                    <p>
                        Un multiprocesador puede verse como un computador paralelo compuesto por varios procesadores interconectados que comparten un mismo sistema de memoria.

Los sistemas multiprocesadores son arquitecturas MIMD con memoria compartida. Tienen un único espacio de direcciones para todos los procesadores y los mecanismos de comunicación se basan en el paso de mensajes desde el punto de vista del programador.

Dado que los multiprocesadores comparten diferentes módulos de memoria, pudiendo acceder a un mismo módulo varios procesadores, a los multiprocesadores también se les llama sistemas de memoria compartida.



Dependiendo de la forma en que los procesadores comparten la memoria, se clasifican en sistemas multiprocesador UMA, NUMA y COMA.
                    </p>
                    <h3 id="redes de interconexion dinamica (indirecta) medio compatido conmutadas"><span>4.3.1</span>Redes de interconexion dinamica (indirecta) medio compatido conmutadas</h3>
                    <p> 
                        Una red dinámica es una red cuya topología puede variar durante el curso de la
ejecución de un programa paralelo o entre dos ejecuciones de programas. La red está
constituida por elementos materiales específicos, llamados commutadores o switches.
Las redes dinámicas se utilizan sobre todo en los multiprocesadores. En este caso, la red
une los procesadores a los bancos de memoria central. Cualquier acceso de un
procesador a la memoria (bien sea para acceder a los datos o a las instrucciones) debe
pasar a través de la red, por lo se dice que la red tiene un acoplamiento fuerte. La red
debe poseer un rendimiento extremadamente bueno para no demorar demasiado a los
procesadores que acceden a memoria.
                    </p>
                  
                    
                </div>
            </div>
        </section>

        <section class="contenedor sobre-el-curso">
            
            <div class="contenedor-sobre-el-curso">
             
                <div class="contenido-textos">
                 
                    <h3 id="sistemas de memoria distribuida multicomputadores"><span>4.4</span>Sistemas de memoria distribuida multicomputadores</h3>
                    <p>
                        Los sistemas de memoria distribuida o multicomputadores pueden ser de dos tipos básicos. El primer de ellos consta de un único computador con múltiples CPUs comunicadas por un bus de datos mientras que en el segundo se utilizan múltiples computadores, cada uno con su propio procesador, enlazados por una red de interconexión más o menos rápida.

                        Sobre los sistemas de multicomputadores de memoria distribuida, se simula memorias compartidas. Se usan los mecanismos de comunicación y sincronización de sistemas multiprocesadores.
                        
                        Un clúster es un tipo de arquitectura paralela distribuida que consiste de un conjunto de computadores independientes interconectados operando de forma conjunta como único recurso computacional sin embargo, cada computador puede utilizarse de forma independiente o separada.
                        
                        En esta arquitectura, el computador paralelo es esencialmente una colección de procesadores secuenciales, cada uno con su propia memoria local, que pueden trabajar conjuntamente.

                    </p>
                    <h3 id="redes de interconexion estaticas"><span>4.4.1</span>Redes de interconexion estaticas</h3>
                    <p> 
                        Una red estática es una red cuya topología
                        queda definida de manera definitiva y estable durante la construcción de la máquina
                        paralela. La red simplemente une los diversos elementos de acuerdo a una configuración
                        dada. Se utiliza sobre todo en el caso de los multicomputadores para conectar los
                        diversos procesadores que posee la máquina. Por la red sólo circulan los mensajes entre
                        procesadores, por lo que se dice que la red presenta un acoplamiento débil. En general,
                        en las redes estáticas se exige poca carga a la red.
                    </p>
                  
                    
                </div>
            </div>
        </section>

       

        
    </main>
    <footer>
        <div class="contenedor-footer">
            <div class="content-foo">
                <h4>Telefono</h4>
                <p>8442323069</p>
            </div>
            <div class="content-foo">
                <h4>Correo</h4>
                <p>gonzalezaguila.derek@gmail.com</p>
            </div>
            <div class="content-foo">
                <h4>Instituto</h4>
                <p>Instituto tecnologico de saltillo</p>
            </div>
        </div>
        <h2 class="titulo-final">&copy; Derek Emir Gonzalez Aguilar | Sistemas Computacionales</h2>
    </footer>
</body>

</html>
